#!/usr/bin/env bash
#
# judge  ‚Äì  AI-powered pull request reviewer using local LLM
#
# Usage:  judge <pr> [options]
#   pr        PR number or branch name
#
# Options:
#   --model MODEL      Use specific model (default: from config)
#   --config FILE      Use custom config file (default: ~/.agentyard/judge.yaml)
#   --init-config      Create default config file and exit
#   -h, --help         Show this help message
#
# Examples:
#   judge 45                           # Review PR #45
#   judge feature/new-login            # Review PR by branch name
#   judge 45 --model mistral-small     # Use specific model
#   judge --init-config                # Create default configuration
#
# Dependencies: gh, python3, llama-cpp-python
# Configuration: ~/.agentyard/judge.yaml
#
# Model Path Resolution (in order of precedence):
#   1. Per-model path in config file
#   2. $AGENTYARD_MODELS_PATH environment variable
#   3. models_dir in config file
#   4. ~/.agentyard/models/
#
set -euo pipefail

# Detect if we're in a virtual environment and find system Python
# This ensures judge uses a consistent Python environment regardless of
# whether the user has activated a project-specific virtual environment
if [[ -n "${VIRTUAL_ENV:-}" ]] || [[ -n "${CONDA_PREFIX:-}" ]]; then
  # We're in a virtual environment - find system Python
  # Try common locations for system Python
  for python_path in /usr/bin/python3 /usr/local/bin/python3 /opt/homebrew/bin/python3 "$HOME/miniforge3/bin/python3" "$HOME/miniconda3/bin/python3"; do
    if [[ -x "$python_path" ]]; then
      # Found a system Python
      JUDGE_PYTHON="$python_path"
      JUDGE_PIP="$python_path -m pip"
      break
    fi
  done
  
  # If we couldn't find a system Python, fall back to whatever python3 is available
  if [[ -z "${JUDGE_PYTHON:-}" ]]; then
    echo "‚ö†Ô∏è  Warning: Running in virtual environment but couldn't find system Python" >&2
    echo "   Using default python3 which may cause dependency issues" >&2
    JUDGE_PYTHON="python3"
    JUDGE_PIP="python3 -m pip"
  else
    echo "üì¶ Detected virtual environment - using system Python: $JUDGE_PYTHON" >&2
  fi
else
  # Not in a virtual environment, use default
  JUDGE_PYTHON="python3"
  JUDGE_PIP="python3 -m pip"
fi

prog=$(basename "$0")
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Default configuration
DEFAULT_CONFIG="$HOME/.agentyard/judge.yaml"
DEFAULT_MODEL="mistralai/Mistral-7B-Instruct-v0.3"
MAX_DIFF_LINES=3000
FORCE_MODE=false

# Initialize command flags
init_config=false
scan_models=false
pr_input=""

# Colors for output
RED='\033[0;31m'
YELLOW='\033[0;33m'
GREEN='\033[0;32m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

usage() {
  cat <<EOF
Usage: $prog <pr> [options]
       $prog --init-config
       $prog scan-models

  pr        PR number or branch name

Options:
  --model MODEL      Use specific model (default: from config)
  --config FILE      Use custom config file (default: ~/.agentyard/judge.yaml)
  --init-config      Create default config file and exit
  scan-models        Scan model directories and update config
  --force            Skip all confirmations (non-interactive mode)
  -h, --help         Show this help message

Examples:
  $prog 45                           # Review PR #45
  $prog feature/new-login            # Review PR by branch name
  $prog 45 --model mistralai/mistral-7b  # Use specific model
  $prog --init-config                # Create default configuration
  $prog scan-models                  # Scan for models and update config

Configuration:
  Default config file: ~/.agentyard/judge.yaml
  Model path resolution (in order):
    1. Per-model path in config
    2. \$AGENTYARD_MODELS_PATH/<namespace>/<model>/
    3. Config models_dir setting
    4. ~/.agentyard/models/<namespace>/<model>/
EOF
}

# ---- Parse arguments --------------------------------------------------------
if [[ $# -eq 0 || ${1:-} == "-h" || ${1:-} == "--help" ]]; then
  usage
  exit 0
fi

# Handle --init-config and scan-models separately
if [[ ${1:-} == "--init-config" ]]; then
  init_config=true
  shift
  # Check if next arg is --force, if so keep it for later
  if [[ ${1:-} == "--force" ]]; then
    FORCE_MODE=true
    shift
  fi
  # Now check for custom config file
  config_file="${1:-$DEFAULT_CONFIG}"
elif [[ ${1:-} == "scan-models" ]]; then
  scan_models=true
  shift
else
  init_config=false
  scan_models=false
  pr_input=$1
  shift
fi

config_file="${config_file:-$DEFAULT_CONFIG}"
model=""

while [[ $# -gt 0 ]]; do
  case $1 in
    --model)
      model="$2"
      shift 2
      ;;
    --config)
      config_file="$2"
      shift 2
      ;;
    --force)
      FORCE_MODE=true
      shift
      ;;
    *)
      echo "Error: Unknown option: $1" >&2
      usage
      exit 1
      ;;
  esac
done

# ---- Check dependencies -----------------------------------------------------
echo "üîç Checking dependencies..."

for cmd in gh; do
  if ! command -v "$cmd" >/dev/null; then
    echo -e "${RED}Error: $cmd not installed.${NC}" >&2
    echo "Please install required dependencies:" >&2
    echo "  - gh: https://cli.github.com/" >&2
    echo "  - python3: https://www.python.org/" >&2
    exit 1
  fi
done

# Check Python separately using our detected Python
if ! command -v "$JUDGE_PYTHON" >/dev/null; then
  echo -e "${RED}Error: Python not found at $JUDGE_PYTHON${NC}" >&2
  echo "Please install Python 3:" >&2
  echo "  - https://www.python.org/" >&2
  exit 1
fi

# Check and install all Python dependencies BEFORE any imports
echo "üì¶ Checking Python dependencies..."

# Create a temporary Python script to check all dependencies at once
# Pass JUDGE_PYTHON as environment variable
JUDGE_PYTHON="$JUDGE_PYTHON" $JUDGE_PYTHON - <<'EOF' || exit 1
import subprocess
import sys
import os

# List of required packages
required_packages = [
    ('yaml', 'PyYAML'),
    ('requests', 'requests'),
    ('psutil', 'psutil'),
    ('llama_cpp', 'llama-cpp-python')
]

# Check which packages are missing
missing_packages = []
for module_name, package_name in required_packages:
    try:
        __import__(module_name)
    except ImportError:
        missing_packages.append((module_name, package_name))

# Install missing packages
if missing_packages:
    # First check if pip is available
    try:
        subprocess.run([sys.executable, '-m', 'pip', '--version'], 
                      check=True, capture_output=True, text=True)
    except subprocess.CalledProcessError:
        print(f"\n‚ùå Error: pip is not available for {sys.executable}", file=sys.stderr)
        print(f"Python path: {sys.executable}", file=sys.stderr)
        print("\nTo fix this:", file=sys.stderr)
        print("1. Install pip for this Python:", file=sys.stderr)
        print(f"   {sys.executable} -m ensurepip", file=sys.stderr)
        print("2. Or use a different Python with pip installed", file=sys.stderr)
        print("3. Or manually install packages:", file=sys.stderr)
        for _, package_name in missing_packages:
            print(f"   - {package_name}", file=sys.stderr)
        sys.exit(1)
    
    print("Installing missing Python packages...")
    for module_name, package_name in missing_packages:
        print(f"  - Installing {package_name}...")
        
        # Try different installation methods
        install_methods = [
            # First try with --user flag
            ([sys.executable, '-m', 'pip', 'install', package_name, '--upgrade', '--user'], None),
            # If that fails due to externally managed env, try with break-system-packages
            ([sys.executable, '-m', 'pip', 'install', package_name, '--upgrade', '--break-system-packages', '--user'], None)
        ]
        
        result = None
        for cmd, env_override in install_methods:
            # Special handling for llama-cpp-python on macOS
            if package_name == 'llama-cpp-python' and sys.platform == 'darwin':
                env = {**subprocess.os.environ, 'CMAKE_ARGS': '-DLLAMA_METAL=on'}
            else:
                env = subprocess.os.environ.copy()
                
            if env_override:
                env.update(env_override)
                
            result = subprocess.run(cmd, env=env, capture_output=True, text=True)
            
            if result.returncode == 0:
                break
            elif "externally-managed-environment" not in result.stderr:
                # If it's not the externally managed error, don't try other methods
                break
        
        if result.returncode != 0:
            print(f"Error: Failed to install {package_name}", file=sys.stderr)
            print(f"Error output: {result.stderr}", file=sys.stderr)
            print("\nSuggestions:", file=sys.stderr)
            print("1. Try installing manually:", file=sys.stderr)
            print(f"   pip install {package_name}", file=sys.stderr)
            print("2. If using conda/mamba, try:", file=sys.stderr)
            print(f"   conda install -c conda-forge {package_name}", file=sys.stderr)
            print("3. Check your Python environment:", file=sys.stderr)
            print(f"   which python3: {sys.executable}", file=sys.stderr)
            print("4. For macOS with brew Python issues, try:", file=sys.stderr)
            print("   brew install python@3.11 && brew link python@3.11", file=sys.stderr)
            sys.exit(1)
        else:
            print(f"  ‚úÖ {package_name} installed successfully")
else:
    print("‚úÖ All Python dependencies are installed")

# Final verification - try to import all modules
print("\nüìã Verifying imports...")
failed_imports = []
for module_name, package_name in required_packages:
    try:
        __import__(module_name)
        print(f"  ‚úÖ {module_name} imports successfully")
    except ImportError as e:
        failed_imports.append((module_name, package_name, str(e)))
        print(f"  ‚ùå {module_name} import failed: {e}")

if failed_imports:
    print("\n‚ùå Some modules failed to import even after installation.", file=sys.stderr)
    print("This usually means packages were installed in a different Python environment.", file=sys.stderr)
    print("\nYour Python interpreter:", file=sys.stderr)
    print(f"  {sys.executable}", file=sys.stderr)
    print("\nTo fix this:", file=sys.stderr)
    print("1. Check which Python is being used:", file=sys.stderr)
    print("   which python3", file=sys.stderr)
    print("2. Install packages for the correct Python:", file=sys.stderr)
    for module_name, package_name, error in failed_imports:
        judge_python = os.environ.get('JUDGE_PYTHON', sys.executable)
        print(f"   {judge_python} -m pip install {package_name}", file=sys.stderr)
    print("3. Or use conda/mamba if you're in a conda environment", file=sys.stderr)
    sys.exit(1)

print("\n‚úÖ Dependency check complete - all modules import successfully!")
EOF

# ---- Handle --init-config ---------------------------------------------------
if [[ "$init_config" == "true" ]]; then
  echo "üìù Creating configuration file..."
  
  # Add Python path for our modules
  export PYTHONPATH="${SCRIPT_DIR}/../lib:${PYTHONPATH:-}"
  
  $JUDGE_PYTHON - "$config_file" "$FORCE_MODE" <<'EOF'
import sys
import os
from pathlib import Path

# Import yaml here after dependencies are installed
try:
    import yaml
except ImportError:
    print("Error: PyYAML is not installed. This should have been installed earlier.", file=sys.stderr)
    sys.exit(1)

# Import model_manager after yaml is available
try:
    from judge.model_manager import create_default_config
except ImportError as e:
    print(f"Error importing model_manager: {e}", file=sys.stderr)
    print("Make sure the PYTHONPATH is set correctly.", file=sys.stderr)
    sys.exit(1)

config_path = Path(sys.argv[1])

# Check if config already exists
if config_path.exists():
    force_mode = sys.argv[2] if len(sys.argv) > 2 else "false"
    if force_mode == "true":
        print(f"Force mode enabled. Overwriting existing config at {config_path}.")
    elif sys.stdin.isatty():
        response = input(f"Config file already exists at {config_path}. Overwrite? [y/N]: ").strip().lower()
        if response != 'y':
            print("Configuration creation cancelled.")
            sys.exit(0)
    else:
        print(f"Config file already exists at {config_path}. Use --force to overwrite.")
        print("Configuration creation cancelled.")
        sys.exit(0)

# Create directory if needed
config_path.parent.mkdir(parents=True, exist_ok=True)

# Create default config
config = create_default_config(config_path)

# Write config file
with open(config_path, 'w') as f:
    yaml.dump(config, f, default_flow_style=False, sort_keys=False)

print(f"‚úÖ Configuration created at: {config_path}")
print(f"\nDefault model: {config['model']['name']}")
print(f"Models directory: {config['models_dir']}")
print("\nYou can now:")
print("  1. Set AGENTYARD_MODELS_PATH environment variable")
print("  2. Edit the config to set custom model paths")
print("  3. Run 'judge <pr>' to review a pull request")
EOF
  exit $?
fi

# ---- Handle scan-models -----------------------------------------------------
if [[ "$scan_models" == "true" ]]; then
  echo "üîç Scanning for models..."
  
  # Create config file if it doesn't exist
  if [[ ! -f "$config_file" ]]; then
    echo "Creating default configuration at $config_file..."
    mkdir -p "$(dirname "$config_file")"
    
    # Add Python path for our modules
    export PYTHONPATH="${SCRIPT_DIR}/../lib:${PYTHONPATH:-}"
    
    $JUDGE_PYTHON - <<EOF
import sys
try:
    import yaml
    from pathlib import Path
    from judge.model_manager import create_default_config
    
    config_path = Path('$config_file')
    config_path.parent.mkdir(parents=True, exist_ok=True)
    config = create_default_config(config_path)
    
    with open(config_path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False, sort_keys=False)
    print(f'‚úÖ Configuration created at: {config_path}')
except ImportError as e:
    print(f"Error during config creation: {e}", file=sys.stderr)
    sys.exit(1)
except Exception as e:
    print(f"Unexpected error: {e}", file=sys.stderr)
    sys.exit(1)
EOF
  fi
  
  # Add Python path for our modules
  export PYTHONPATH="${SCRIPT_DIR}/../lib:${PYTHONPATH:-}"
  
  # Run the model scanner
  $JUDGE_PYTHON - "$config_file" <<'EOF'
import sys
import os
import json
from pathlib import Path
from glob import glob

# Import yaml with error handling
try:
    import yaml
except ImportError:
    print("Error: PyYAML is not installed. This should have been installed earlier.", file=sys.stderr)
    sys.exit(1)

# Import after yaml is available
from judge.model_manager import ModelManager

config_path = Path(sys.argv[1])

# Load existing config
with open(config_path, 'r') as f:
    config = yaml.safe_load(f) or {}

# Initialize paths to scan
scan_paths = []

# 1. Environment variable (highest priority)
env_path = os.environ.get('AGENTYARD_MODELS_PATH')
if env_path:
    scan_paths.append(Path(os.path.expanduser(env_path)))

# 2. Config models_dir
if 'models_dir' in config:
    scan_paths.append(Path(os.path.expanduser(config['models_dir'])))

# 3. Default path
scan_paths.append(Path.home() / ".agentyard" / "models")

# 4. LM Studio paths
lm_studio_paths = [
    Path.home() / ".cache" / "lm-studio" / "models",
    Path("~/.lmstudio/models").expanduser()
]
for path in lm_studio_paths:
    if path.exists():
        scan_paths.append(path)

# Remove duplicates while preserving order
seen = set()
unique_paths = []
for path in scan_paths:
    if path not in seen:
        seen.add(path)
        unique_paths.append(path)

print(f"üìÇ Scanning directories:")
for path in unique_paths:
    print(f"   - {path}")

# Import metadata reader
from judge.model_manager import GGUFMetadataReader

# Scan for models
discovered_models = {}
metadata_cache = {}
total_found = 0

for base_path in unique_paths:
    if not base_path.exists():
        continue
    
    # Look for namespace/model structure
    try:
        namespace_dirs = list(base_path.iterdir())
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Error reading {base_path}: {e}")
        continue
        
    for namespace_dir in namespace_dirs:
        if not namespace_dir.is_dir():
            continue
        
        # Skip hidden directories and common non-model directories
        if namespace_dir.name.startswith('.') or namespace_dir.name in ['cache', 'tmp', 'temp']:
            continue
            
        try:
            model_dirs = list(namespace_dir.iterdir())
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Error reading {namespace_dir}: {e}")
            continue
            
        for model_dir in model_dirs:
            if not model_dir.is_dir():
                continue
                
            # Skip hidden directories
            if model_dir.name.startswith('.'):
                continue
                
            # Look for GGUF files (with timeout protection)
            try:
                gguf_files = []
                for f in model_dir.glob("*.gguf"):
                    gguf_files.append(f)
                    if len(gguf_files) > 10:  # Limit files per model
                        break
                        
                # Also check for safetensors files (LM Studio format)
                safetensors_files = []
                for f in model_dir.glob("*.safetensors"):
                    safetensors_files.append(f)
                    if len(safetensors_files) > 10:  # Limit files per model
                        break
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Error scanning {model_dir}: {e}")
                continue
            
            model_files = gguf_files + safetensors_files
            
            if model_files:
                model_name = f"{namespace_dir.name}/{model_dir.name}"
                
                # Use the first model file found
                model_file = model_files[0]
                
                # Skip if already in config with custom path
                if model_name in config.get('models', {}) and 'path' in config['models'][model_name]:
                    print(f"‚è≠Ô∏è  Skipping {model_name} (custom path already configured)")
                    continue
                
                discovered_models[model_name] = {
                    'path': str(model_file),
                    'directory': str(model_dir),
                    'files': [f.name for f in model_files]
                }
                
                # Try to read GGUF metadata
                if model_file.suffix == '.gguf':
                    try:
                        reader = GGUFMetadataReader(model_file)
                        metadata = reader.read_metadata()
                        metadata_cache[model_name] = metadata
                        quant = metadata.get('quantization', 'Unknown')
                        size_gb = metadata.get('file_size_gb', 0)
                        print(f"‚úÖ Found {model_name} ({quant}, {size_gb:.1f}GB)")
                    except Exception as e:
                        print(f"‚úÖ Found {model_name} (metadata read failed: {e})")
                else:
                    # For safetensors, just note it was found
                    size_gb = model_file.stat().st_size / (1024 ** 3)
                    print(f"‚úÖ Found {model_name} (safetensors, {size_gb:.1f}GB)")
                    metadata_cache[model_name] = {
                        'file_type': 'safetensors',
                        'file_size_gb': size_gb,
                        'architecture': 'unknown'
                    }
                
                total_found += 1

print(f"\nüìä Found {total_found} models")

if total_found > 0:
    # Update config with discovered models
    if 'models' not in config:
        config['models'] = {}
    
    # Remove old flat models that don't follow namespace/model pattern
    old_models = []
    for model_name in list(config['models'].keys()):
        if '/' not in model_name:
            old_models.append(model_name)
    
    if old_models:
        print(f"\nüßπ Removing {len(old_models)} old flat model entries:")
        for model in old_models:
            print(f"   - {model}")
            del config['models'][model]
    
    # Add discovered models
    for model_name, model_info in discovered_models.items():
        if model_name not in config['models']:
            config['models'][model_name] = {}
        # Note: We don't set the path here since the model manager
        # will find it automatically using the namespace/model structure
    
    # Write updated config
    with open(config_path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False, sort_keys=False)
    
    # Write metadata cache
    cache_dir = config_path.parent / '.cache'
    cache_dir.mkdir(exist_ok=True)
    metadata_cache_file = cache_dir / 'model_metadata.json'
    
    # Load existing cache if it exists
    existing_cache = {}
    if metadata_cache_file.exists():
        try:
            with open(metadata_cache_file, 'r') as f:
                existing_cache = json.load(f)
        except:
            pass
    
    # Merge with new metadata
    existing_cache.update(metadata_cache)
    
    # Write updated cache
    with open(metadata_cache_file, 'w') as f:
        json.dump(existing_cache, f, indent=2)
    
    print(f"\n‚úÖ Updated configuration at: {config_path}")
    print(f"‚úÖ Updated metadata cache at: {metadata_cache_file}")
    print("\nDiscovered models:")
    for model_name in sorted(discovered_models.keys()):
        metadata = metadata_cache.get(model_name, {})
        quant = metadata.get('quantization', 'Unknown')
        size = metadata.get('file_size_gb', 0)
        print(f"  - {model_name} ({quant}, {size:.1f}GB)")
else:
    print("\nNo models found. Make sure models are stored in namespace/model folder structure.")
    print("Example: ~/.agentyard/models/mistralai/mistral-7b/mistral-7b-instruct-v0.2.Q4_K_M.gguf")

EOF
  exit $?
fi

# ---- Check GitHub authentication --------------------------------------------
if ! gh auth status >/dev/null 2>&1; then
  echo -e "${RED}Error: Not authenticated with GitHub CLI${NC}" >&2
  echo "Please run: gh auth login" >&2
  exit 1
fi

# ---- Determine PR number from input -----------------------------------------
echo "üìã Resolving pull request..."

# Check if input is a number
if [[ "$pr_input" =~ ^[0-9]+$ ]]; then
  pr_number="$pr_input"
else
  # Try to find PR by branch name
  pr_number=$(gh pr list --state open --head "$pr_input" --json number --jq '.[0].number // empty' 2>/dev/null || true)
  
  if [[ -z "$pr_number" ]]; then
    echo -e "${RED}Error: No open PR found for branch '$pr_input'${NC}" >&2
    echo "Try using a PR number instead." >&2
    exit 1
  fi
  
  echo "Found PR #$pr_number for branch '$pr_input'"
fi

# ---- Create default config if it doesn't exist ------------------------------
if [[ ! -f "$config_file" ]]; then
  echo -e "${YELLOW}Configuration file not found at $config_file${NC}"
  echo "Creating default configuration..."
  
  # Create config directory if needed
  mkdir -p "$(dirname "$config_file")"
  
  # Add Python path for our modules
  export PYTHONPATH="${SCRIPT_DIR}/../lib:${PYTHONPATH:-}"
  
  # Create default config
  if ! $JUDGE_PYTHON - "$config_file" <<'EOF'
import sys
try:
    import yaml
    from pathlib import Path
    from judge.model_manager import create_default_config
    
    config_path = Path(sys.argv[1])
    config = create_default_config(config_path)
    
    with open(config_path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False, sort_keys=False)
    
    print(f"‚úÖ Created default configuration at: {config_path}")
    print("")
    print("You can now:")
    print("  1. Set AGENTYARD_MODELS_PATH environment variable")
    print("  2. Edit the config to set custom model paths")
    print("  3. Run 'judge scan-models' to discover existing models")
    print("  4. Run judge again to review a pull request")
except Exception as e:
    print(f"Error creating config: {e}", file=sys.stderr)
    sys.exit(1)
EOF
  then
    echo -e "${RED}Failed to create default configuration${NC}" >&2
    echo "Please run: judge --init-config" >&2
    exit 1
  fi
fi

# ---- Validate model early ---------------------------------------------------
echo "üîç Validating model..."

# Add Python path for our modules
export PYTHONPATH="${SCRIPT_DIR}/../lib:${PYTHONPATH:-}"

# Determine which model to use
if [[ -n "$model" ]]; then
  model_name="$model"
else
  # Extract model name from config
  model_name=$($JUDGE_PYTHON - "$config_file" <<'EOF' 2>/dev/null || echo "mistralai/mistral-small-2409"
import sys
try:
    import yaml
    config_file = sys.argv[1]
    with open(config_file, 'r') as f:
        config = yaml.safe_load(f)
        print(config.get('model', {}).get('name', 'mistralai/Mistral-7B-Instruct-v0.3'))
except:
    print('mistralai/Mistral-7B-Instruct-v0.3')
EOF
  )
fi

# Validate and download model if needed
$JUDGE_PYTHON - "$config_file" "$model_name" "$FORCE_MODE" <<'EOF' || exit 1
import sys
from judge.model_manager import ModelManager

config_path = sys.argv[1]
model_name = sys.argv[2]
force_mode = sys.argv[3] == "true"

manager = ModelManager(config_path)
success, model_path = manager.validate_and_download_model(model_name, force=force_mode)

if not success:
    print(f"‚ùå Failed to validate/download model: {model_name}")
    sys.exit(1)

print(f"‚úÖ Model validated: {model_path}")
EOF

# ---- Create Python helper script --------------------------------------------
helper_script="$SCRIPT_DIR/judge-ai.py"
echo "Creating AI helper script..."
cat > "$helper_script" <<'EOF'
#!/usr/bin/env python3
"""
judge-ai.py - AI helper for the judge command
Handles LLM loading and inference for PR reviews
"""

import sys
import json
import yaml
import os
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
import argparse

# Add parent directory to path for our modules
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'lib'))

try:
    from llama_cpp import Llama
except ImportError:
    print("Error: llama-cpp-python not installed", file=sys.stderr)
    sys.exit(1)

try:
    from judge.model_manager import ModelManager
except ImportError:
    print("Error: Could not import model manager", file=sys.stderr)
    sys.exit(1)


def load_config(config_path: str) -> Dict[str, Any]:
    """Load configuration from YAML file"""
    try:
        config_path = os.path.expanduser(config_path)
        with open(config_path, 'r') as f:
            return yaml.safe_load(f) or {}
    except Exception as e:
        print(f"Error loading config: {e}", file=sys.stderr)
        # Return default config
        return {
            "model": {
                "name": "mistral-small-2409",
                "context_size": 32768,
                "gpu_layers": -1,
                "temperature": 0.1,
                "max_tokens": 4096
            },
            "review": {
                "max_diff_lines": 3000,
                "include_pr_description": True,
                "output_format": "markdown"
            }
        }


def estimate_tokens(text: str) -> int:
    """Rough estimate of token count (4 chars per token average)"""
    return len(text) // 4

def create_review_prompt(pr_data: Dict[str, Any], diff_content: str, context_size: int = 32768) -> Tuple[str, str]:
    """Create the prompt for code review"""
    system_prompt = """You are an expert code reviewer called "Judge". Your role is to:
1. Identify bugs, security issues, and logic errors
2. Suggest improvements for code quality and maintainability
3. Acknowledge good practices and well-written code
4. Focus on actionable, specific feedback
5. Be constructive and professional in tone

Review the following pull request and provide detailed feedback."""

    pr_title = pr_data.get('title', 'Untitled PR')
    pr_description = pr_data.get('body', 'No description provided')
    
    # Truncate description if too long
    if len(pr_description) > 1000:
        pr_description = pr_description[:997] + "..."
    
    # Build user prompt
    user_prompt_base = f"""PR Title: {pr_title}
Description: {pr_description}

Changed Files:
{', '.join(pr_data.get('files', []))}

Diff:
"""
    
    # Estimate token usage
    base_tokens = estimate_tokens(system_prompt + user_prompt_base)
    diff_tokens = estimate_tokens(diff_content)
    response_tokens = 4096  # Reserve for response
    
    total_needed = base_tokens + diff_tokens + response_tokens
    
    if total_needed > context_size * 0.9:  # Leave 10% buffer
        # Need to truncate diff
        available_for_diff = int((context_size * 0.9) - base_tokens - response_tokens)
        max_diff_chars = available_for_diff * 4  # Convert back to chars
        
        if max_diff_chars < len(diff_content):
            print(f"‚ö†Ô∏è  Warning: Diff too large for context ({diff_tokens} tokens). Truncating to fit.", file=sys.stderr)
            diff_content = diff_content[:max_diff_chars] + "\n\n... (diff truncated for context window)"
    
    user_prompt = user_prompt_base + diff_content + """

Please provide a comprehensive code review with:
- Summary of critical issues, important concerns, and suggestions
- Detailed file-by-file analysis with line references
- Positive feedback on well-written code
- Actionable recommendations for improvements"""

    return system_prompt, user_prompt


def format_review_output(response: str) -> str:
    """Format the AI response into structured markdown"""
    # The model should already return well-formatted markdown,
    # but we can add additional formatting if needed
    
    if not response.strip().startswith("#"):
        # Add header if missing
        response = f"## AI Code Review\n\n{response}"
    
    return response


def run_review(config_path: str, pr_data_json: str, diff_content: str, 
               model_override: Optional[str] = None) -> None:
    """Run the AI review on the PR"""
    try:
        # Load configuration
        config = load_config(config_path)
        model_config = config.get('model', {})
        
        # Determine model name
        model_name = model_override or model_config.get('name', 'mistralai/mistral-small-2409')
        
        # Use ModelManager to get model path
        manager = ModelManager(config_path)
        model_dir = manager.get_model_path(model_name)
        
        # Find the GGUF file in the model directory
        if model_dir.exists() and model_dir.is_dir():
            gguf_files = list(model_dir.glob("*.gguf"))
            if gguf_files:
                model_path = gguf_files[0]  # Use the first GGUF file found
            else:
                print(f"Error: No GGUF files found in {model_dir}", file=sys.stderr)
                sys.exit(1)
        else:
            print(f"Error: Model directory not found at {model_dir}", file=sys.stderr)
            print("The model should have been validated earlier. Please check your setup.", file=sys.stderr)
            sys.exit(1)
        
        # Parse PR data
        pr_data = json.loads(pr_data_json)
        
        # Create prompt
        context_size = model_config.get('context_size', 32768)
        system_prompt, user_prompt = create_review_prompt(pr_data, diff_content, context_size)
        
        # Initialize model with progress indicator
        print("ü§ñ Loading AI model...", file=sys.stderr)
        print(f"   Model: {model_path.name}", file=sys.stderr)
        print(f"   Context: {context_size} tokens", file=sys.stderr)
        
        llm = Llama(
            model_path=str(model_path),
            n_ctx=context_size,
            n_gpu_layers=model_config.get('gpu_layers', -1),
            verbose=False
        )
        
        # Run inference
        print("üîç Analyzing code changes...", file=sys.stderr)
        response = llm.create_chat_completion(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=model_config.get('temperature', 0.1),
            max_tokens=model_config.get('max_tokens', 4096),
            stream=False
        )
        
        # Extract and format the response
        review_text = response['choices'][0]['message']['content']
        formatted_review = format_review_output(review_text)
        
        # Output the review
        print(formatted_review)
        
    except Exception as e:
        print(f"Error during AI review: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc(file=sys.stderr)
        sys.exit(1)


def main():
    parser = argparse.ArgumentParser(description='AI helper for judge PR reviewer')
    parser.add_argument('config_path', help='Path to configuration file')
    parser.add_argument('pr_data', help='PR data as JSON')
    parser.add_argument('diff_content', help='Diff content')
    parser.add_argument('--model', help='Model override', default=None)
    
    args = parser.parse_args()
    
    run_review(args.config_path, args.pr_data, args.diff_content, args.model)


if __name__ == '__main__':
    main()
EOF
chmod +x "$helper_script"

# ---- Fetch PR information ---------------------------------------------------
echo "üì• Fetching PR #$pr_number..."

# Get PR metadata
pr_data=$(gh pr view "$pr_number" --json title,body,files,headRefName || {
  echo -e "${RED}Error: Failed to fetch PR information${NC}" >&2
  exit 1
})

# Extract branch name for diff
branch_name=$(echo "$pr_data" | jq -r '.headRefName')
files_changed=$(echo "$pr_data" | jq -r '.files | length')

echo "Branch: $branch_name"
echo "Files changed: $files_changed"

# Get the diff
echo "üìä Fetching diff..."
diff_content=$(gh pr diff "$pr_number" || {
  echo -e "${RED}Error: Failed to fetch PR diff${NC}" >&2
  exit 1
})

# Check diff size
diff_lines=$(echo "$diff_content" | wc -l)
if [[ $diff_lines -gt $MAX_DIFF_LINES ]]; then
  echo -e "${YELLOW}Warning: Large diff ($diff_lines lines). Truncating to $MAX_DIFF_LINES lines.${NC}"
  
  # Smart truncation: try to keep complete file changes
  # Count the number of files and lines per file
  file_count=$(echo "$diff_content" | grep -c "^diff --git" || echo "1")
  
  if [[ $file_count -gt 1 ]]; then
    # Multiple files - try to include complete files up to the limit
    echo "  Multiple files detected. Attempting smart truncation..."
    
    # Create a temporary file to build truncated diff
    temp_diff=""
    current_lines=0
    
    # Process each file diff
    while IFS= read -r line; do
      if [[ "$line" =~ ^diff\ --git ]]; then
        # New file starts - check if we have room
        next_file_lines=$(echo "$diff_content" | sed -n "/$line/,/^diff --git/p" | wc -l)
        
        if [[ $((current_lines + next_file_lines)) -gt $MAX_DIFF_LINES ]]; then
          # Would exceed limit - stop here
          break
        fi
      fi
      
      temp_diff="${temp_diff}${line}"$'\n'
      ((current_lines++))
      
      if [[ $current_lines -ge $MAX_DIFF_LINES ]]; then
        break
      fi
    done <<< "$diff_content"
    
    diff_content="$temp_diff"
    echo "  Truncated to $current_lines lines, preserving complete file boundaries where possible."
  else
    # Single file or fallback - simple truncation
    diff_content=$(echo "$diff_content" | head -n "$MAX_DIFF_LINES")
  fi
fi

# ---- Run AI review ----------------------------------------------------------
echo "üöÄ Starting AI review..."

# Prepare files list
files_list=$(echo "$pr_data" | jq -r '.files[].path' | paste -sd, -)
pr_data_with_files=$(echo "$pr_data" | jq --arg files "$files_list" '. + {files: ($files | split(","))}')

# Run the Python helper
# First, let's add some debugging info
echo "üìù Preparing AI review with $(echo "$diff_content" | wc -l) lines of diff..."
echo "   Model: ${model:-default from config}"
echo "   Files: $files_list"

# Run with error capture and timeout
# Set a timeout of 5 minutes for the AI review
timeout_cmd="timeout"
if command -v gtimeout >/dev/null 2>&1; then
  timeout_cmd="gtimeout"  # macOS with coreutils
fi

review_output=$($timeout_cmd 300 $JUDGE_PYTHON "$helper_script" \
  "$config_file" \
  "$pr_data_with_files" \
  "$diff_content" \
  ${model:+--model "$model"} 2>&1) || {
    exit_code=$?
    
    if [[ $exit_code -eq 124 ]] || [[ $exit_code -eq 143 ]]; then
      # Timeout occurred
      echo -e "${RED}Error: AI review timed out after 5 minutes${NC}" >&2
      echo -e "${YELLOW}This usually means:${NC}" >&2
      echo "1. The model is too large for your system" >&2
      echo "2. The PR diff is too complex" >&2
      echo "3. Insufficient RAM or CPU resources" >&2
      echo "" >&2
      echo -e "${YELLOW}Try these solutions:${NC}" >&2
      echo "1. Use a smaller/faster model:" >&2
      echo "   judge $pr_number --model lmstudio-community/mathstral-7B-v0.1-GGUF" >&2
      echo "2. Run 'judge scan-models' to see available models" >&2
      echo "3. Close other applications to free up RAM" >&2
    else
      echo -e "${RED}Error: AI review failed (exit code: $exit_code)${NC}" >&2
      echo -e "${RED}Error output:${NC}" >&2
      echo "$review_output" >&2
      
      # Common troubleshooting tips
      echo -e "\n${YELLOW}Troubleshooting tips:${NC}" >&2
      echo "1. Check if the model file exists and is accessible" >&2
      echo "2. Ensure you have enough RAM for the model (8GB+ recommended)" >&2
      echo "3. Try with a smaller model using --model flag" >&2
      echo "4. Check the log output above for specific errors" >&2
    fi
    
    exit 1
  }

# ---- Output the review ------------------------------------------------------
echo
echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
echo
echo "$review_output"
echo
echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
echo
echo -e "${GREEN}‚úÖ Review complete!${NC}"

# Optional: Save review to file
if [[ -n "${JUDGE_SAVE_REVIEWS:-}" ]]; then
  review_dir="$HOME/.agentyard/reviews"
  mkdir -p "$review_dir"
  review_file="$review_dir/pr-${pr_number}-$(date +%Y%m%d-%H%M%S).md"
  echo "$review_output" > "$review_file"
  echo "Review saved to: $review_file"
fi