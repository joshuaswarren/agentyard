#!/usr/bin/env bash
#
# judge  ‚Äì  AI-powered pull request reviewer using local LLM
#
# Usage:  judge <pr> [options]
#   pr        PR number or branch name
#
# Options:
#   --model MODEL      Use specific model (default: from config)
#   --config FILE      Use custom config file (default: ~/.agentyard/judge.yaml)
#   -h, --help         Show this help message
#
# Examples:
#   judge 45                           # Review PR #45
#   judge feature/new-login            # Review PR by branch name
#   judge 45 --model mistral-small     # Use specific model
#
# Dependencies: gh, python3, llama-cpp-python
# Configuration: ~/.agentyard/judge.yaml
#
set -euo pipefail

prog=$(basename "$0")
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Default configuration
DEFAULT_CONFIG="$HOME/.agentyard/judge.yaml"
DEFAULT_MODEL="mistral-small-2409"
MAX_DIFF_LINES=1000

# Colors for output
RED='\033[0;31m'
YELLOW='\033[0;33m'
GREEN='\033[0;32m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

usage() {
  cat <<EOF
Usage: $prog <pr> [options]

  pr        PR number or branch name

Options:
  --model MODEL      Use specific model (default: from config)
  --config FILE      Use custom config file (default: ~/.agentyard/judge.yaml)
  -h, --help         Show this help message

Examples:
  $prog 45                           # Review PR #45
  $prog feature/new-login            # Review PR by branch name
  $prog 45 --model mistral-small     # Use specific model

Configuration:
  Default config file: ~/.agentyard/judge.yaml
  See documentation for configuration options.
EOF
}

# ---- Parse arguments --------------------------------------------------------
if [[ $# -eq 0 || ${1:-} == "-h" || ${1:-} == "--help" ]]; then
  usage
  exit 0
fi

pr_input=$1
shift

config_file="$DEFAULT_CONFIG"
model=""

while [[ $# -gt 0 ]]; do
  case $1 in
    --model)
      model="$2"
      shift 2
      ;;
    --config)
      config_file="$2"
      shift 2
      ;;
    *)
      echo "Error: Unknown option: $1" >&2
      usage
      exit 1
      ;;
  esac
done

# ---- Check dependencies -----------------------------------------------------
echo "üîç Checking dependencies..."

for cmd in gh python3; do
  if ! command -v "$cmd" >/dev/null; then
    echo -e "${RED}Error: $cmd not installed.${NC}" >&2
    echo "Please install required dependencies:" >&2
    echo "  - gh: https://cli.github.com/" >&2
    echo "  - python3: https://www.python.org/" >&2
    exit 1
  fi
done

# Check for Python dependencies
if ! python3 -c "import llama_cpp" 2>/dev/null; then
  echo -e "${YELLOW}Warning: llama-cpp-python not installed.${NC}" >&2
  echo "Installing llama-cpp-python..." >&2
  
  # Detect platform for Metal support
  if [[ "$(uname)" == "Darwin" ]]; then
    echo "Detected macOS - installing with Metal support..."
    CMAKE_ARGS="-DLLAMA_METAL=on" pip3 install llama-cpp-python --upgrade || {
      echo -e "${RED}Error: Failed to install llama-cpp-python${NC}" >&2
      exit 1
    }
  else
    pip3 install llama-cpp-python --upgrade || {
      echo -e "${RED}Error: Failed to install llama-cpp-python${NC}" >&2
      exit 1
    }
  fi
fi

# Check if PyYAML is installed for config parsing
if ! python3 -c "import yaml" 2>/dev/null; then
  echo "Installing PyYAML for configuration support..."
  pip3 install PyYAML || {
    echo -e "${RED}Error: Failed to install PyYAML${NC}" >&2
    exit 1
  }
fi

# ---- Check GitHub authentication --------------------------------------------
if ! gh auth status >/dev/null 2>&1; then
  echo -e "${RED}Error: Not authenticated with GitHub CLI${NC}" >&2
  echo "Please run: gh auth login" >&2
  exit 1
fi

# ---- Determine PR number from input -----------------------------------------
echo "üìã Resolving pull request..."

# Check if input is a number
if [[ "$pr_input" =~ ^[0-9]+$ ]]; then
  pr_number="$pr_input"
else
  # Try to find PR by branch name
  pr_number=$(gh pr list --state open --head "$pr_input" --json number --jq '.[0].number // empty' 2>/dev/null || true)
  
  if [[ -z "$pr_number" ]]; then
    echo -e "${RED}Error: No open PR found for branch '$pr_input'${NC}" >&2
    echo "Try using a PR number instead." >&2
    exit 1
  fi
  
  echo "Found PR #$pr_number for branch '$pr_input'"
fi

# ---- Create default config if it doesn't exist ------------------------------
if [[ ! -f "$config_file" ]]; then
  echo "Creating default configuration at $config_file..."
  mkdir -p "$(dirname "$config_file")"
  cat > "$config_file" <<'EOF'
# Judge AI PR Reviewer Configuration

model:
  name: "mistral-small-2409"
  # Update this path to your model location
  path: "~/.agentyard/models/mistral-small-2409.gguf"
  context_size: 32768
  gpu_layers: -1  # Use all GPU layers on Metal
  temperature: 0.1
  max_tokens: 4096

review:
  max_diff_lines: 1000
  include_pr_description: true
  output_format: "markdown"

github:
  default_remote: "origin"
EOF
  echo -e "${YELLOW}Please update the model path in $config_file${NC}"
fi

# ---- Create Python helper script --------------------------------------------
helper_script="$SCRIPT_DIR/judge-ai.py"
if [[ ! -f "$helper_script" ]]; then
  echo "Creating AI helper script..."
  cat > "$helper_script" <<'EOF'
#!/usr/bin/env python3
"""
judge-ai.py - AI helper for the judge command
Handles LLM loading and inference for PR reviews
"""

import sys
import json
import yaml
import os
from pathlib import Path
from typing import Dict, Any, Optional
import argparse

try:
    from llama_cpp import Llama
except ImportError:
    print("Error: llama-cpp-python not installed", file=sys.stderr)
    sys.exit(1)


def load_config(config_path: str) -> Dict[str, Any]:
    """Load configuration from YAML file"""
    try:
        config_path = os.path.expanduser(config_path)
        with open(config_path, 'r') as f:
            return yaml.safe_load(f)
    except Exception as e:
        print(f"Error loading config: {e}", file=sys.stderr)
        # Return default config
        return {
            "model": {
                "name": "mistral-small-2409",
                "path": "~/.agentyard/models/mistral-small-2409.gguf",
                "context_size": 32768,
                "gpu_layers": -1,
                "temperature": 0.1,
                "max_tokens": 4096
            },
            "review": {
                "max_diff_lines": 1000,
                "include_pr_description": True,
                "output_format": "markdown"
            }
        }


def create_review_prompt(pr_data: Dict[str, Any], diff_content: str) -> str:
    """Create the prompt for code review"""
    system_prompt = """You are an expert code reviewer called "Judge". Your role is to:
1. Identify bugs, security issues, and logic errors
2. Suggest improvements for code quality and maintainability
3. Acknowledge good practices and well-written code
4. Focus on actionable, specific feedback
5. Be constructive and professional in tone

Review the following pull request and provide detailed feedback."""

    pr_title = pr_data.get('title', 'Untitled PR')
    pr_description = pr_data.get('body', 'No description provided')
    
    user_prompt = f"""PR Title: {pr_title}
Description: {pr_description}

Changed Files:
{', '.join(pr_data.get('files', []))}

Diff:
{diff_content}

Please provide a comprehensive code review with:
- Summary of critical issues, important concerns, and suggestions
- Detailed file-by-file analysis with line references
- Positive feedback on well-written code
- Actionable recommendations for improvements"""

    return system_prompt, user_prompt


def format_review_output(response: str) -> str:
    """Format the AI response into structured markdown"""
    # The model should already return well-formatted markdown,
    # but we can add additional formatting if needed
    
    if not response.strip().startswith("#"):
        # Add header if missing
        response = f"## AI Code Review\n\n{response}"
    
    return response


def run_review(config_path: str, pr_data_json: str, diff_content: str, 
               model_override: Optional[str] = None) -> None:
    """Run the AI review on the PR"""
    try:
        # Load configuration
        config = load_config(config_path)
        model_config = config['model']
        
        # Override model if specified
        if model_override:
            model_config['name'] = model_override
        
        # Expand model path
        model_path = os.path.expanduser(model_config['path'])
        
        # Check if model file exists
        if not os.path.exists(model_path):
            print(f"Error: Model file not found at {model_path}", file=sys.stderr)
            print("Please download the model or update the path in your config.", file=sys.stderr)
            print("\nTo download models:", file=sys.stderr)
            print("  1. Visit https://huggingface.co/", file=sys.stderr)
            print("  2. Search for GGUF models (e.g., 'mistral-small gguf')", file=sys.stderr)
            print("  3. Download a quantized version (e.g., Q4_K_M)", file=sys.stderr)
            print(f"  4. Save to {model_path}", file=sys.stderr)
            sys.exit(1)
        
        # Parse PR data
        pr_data = json.loads(pr_data_json)
        
        # Create prompt
        system_prompt, user_prompt = create_review_prompt(pr_data, diff_content)
        
        # Initialize model with progress indicator
        print("ü§ñ Loading AI model...", file=sys.stderr)
        llm = Llama(
            model_path=model_path,
            n_ctx=model_config.get('context_size', 32768),
            n_gpu_layers=model_config.get('gpu_layers', -1),
            verbose=False
        )
        
        # Run inference
        print("üîç Analyzing code changes...", file=sys.stderr)
        response = llm.create_chat_completion(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=model_config.get('temperature', 0.1),
            max_tokens=model_config.get('max_tokens', 4096),
            stream=False
        )
        
        # Extract and format the response
        review_text = response['choices'][0]['message']['content']
        formatted_review = format_review_output(review_text)
        
        # Output the review
        print(formatted_review)
        
    except Exception as e:
        print(f"Error during AI review: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc(file=sys.stderr)
        sys.exit(1)


def main():
    parser = argparse.ArgumentParser(description='AI helper for judge PR reviewer')
    parser.add_argument('config_path', help='Path to configuration file')
    parser.add_argument('pr_data', help='PR data as JSON')
    parser.add_argument('diff_content', help='Diff content')
    parser.add_argument('--model', help='Model override', default=None)
    
    args = parser.parse_args()
    
    run_review(args.config_path, args.pr_data, args.diff_content, args.model)


if __name__ == '__main__':
    main()
EOF
  chmod +x "$helper_script"
fi

# ---- Fetch PR information ---------------------------------------------------
echo "üì• Fetching PR #$pr_number..."

# Get PR metadata
pr_data=$(gh pr view "$pr_number" --json title,body,files,headRefName || {
  echo -e "${RED}Error: Failed to fetch PR information${NC}" >&2
  exit 1
})

# Extract branch name for diff
branch_name=$(echo "$pr_data" | jq -r '.headRefName')
files_changed=$(echo "$pr_data" | jq -r '.files | length')

echo "Branch: $branch_name"
echo "Files changed: $files_changed"

# Get the diff
echo "üìä Fetching diff..."
diff_content=$(gh pr diff "$pr_number" || {
  echo -e "${RED}Error: Failed to fetch PR diff${NC}" >&2
  exit 1
})

# Check diff size
diff_lines=$(echo "$diff_content" | wc -l)
if [[ $diff_lines -gt $MAX_DIFF_LINES ]]; then
  echo -e "${YELLOW}Warning: Large diff ($diff_lines lines). Truncating to $MAX_DIFF_LINES lines.${NC}"
  diff_content=$(echo "$diff_content" | head -n "$MAX_DIFF_LINES")
fi

# ---- Run AI review ----------------------------------------------------------
echo "üöÄ Starting AI review..."

# Prepare files list
files_list=$(echo "$pr_data" | jq -r '.files[].path' | paste -sd, -)
pr_data_with_files=$(echo "$pr_data" | jq --arg files "$files_list" '. + {files: ($files | split(","))}')

# Run the Python helper
review_output=$(python3 "$helper_script" \
  "$config_file" \
  "$pr_data_with_files" \
  "$diff_content" \
  ${model:+--model "$model"} || {
    echo -e "${RED}Error: AI review failed${NC}" >&2
    exit 1
  })

# ---- Output the review ------------------------------------------------------
echo
echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
echo
echo "$review_output"
echo
echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
echo
echo -e "${GREEN}‚úÖ Review complete!${NC}"

# Optional: Save review to file
if [[ -n "${JUDGE_SAVE_REVIEWS:-}" ]]; then
  review_dir="$HOME/.agentyard/reviews"
  mkdir -p "$review_dir"
  review_file="$review_dir/pr-${pr_number}-$(date +%Y%m%d-%H%M%S).md"
  echo "$review_output" > "$review_file"
  echo "Review saved to: $review_file"
fi